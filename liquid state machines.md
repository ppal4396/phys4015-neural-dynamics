# liquid state machines

See (Maas, 2002).

This computational model allows real-time computation on perturbations.

A liquid state machine $M$ approximates functions that are maps 
- from functions continuous input (or 'perturbations'/'disturbances') $u(\cdot)$ 
- to functions of continuous output $y(\cdot)$.

At every time step $t$, the internal state of the machine $x^M(t)$ represents the current response to all preceding perturbations, $u(s)$ where $s < t$. The internal state is generated by some linear transformation, or 'liquid filter' $L^M$, that is time-invariant.

At any time step, it is possible to transform the internal state $x^M(t)$ to output $y(t)$, via a memoryless function $f^M$ i.e., a function that depends only on the internal liquid state and does not depend on any other form of memory of past inputs.

![[LSM architecture.png|350]]

- $x^M(t) = (L^Mu)(t)$
- $y(y) = f^M(x^M(t))$

Consider that this model would require no attractor in a dynamical system to acquire a stable output.

## universal computation power in LSMs:

An LSM is 'computationally complete' for computations with fading memory, on functions of time, as follows. An LSM has universal computational power, if it is able to approximate  
any map from functions of time $u(·)$ to functions of time $y(·)$, where that map is time invariant and  has fading memory.

## showing an LSM has universal computation power

LSM must satisfy two properties to be computationally complete as above.

**pointwise separation property**:
this is a property of $L^M$, such that for two input functions $u(\cdot)$ and $v(\cdot)$, trajectories are separated. That is, for $v(s), u(s) : s \le 0$, $(L^Mv)(0) \ne (L^Mu)(0)$.

**approximation property (AP):**
this is a property of the class of functions that $f^M$ is drawn from, such that for any continuous function $h: x \in X \subseteq \mathbb{R}^m \mapsto \mathbb{R}$, some function $f$ exists where
$$|h(x) - f(x)| \le \rho : \ \forall x \in X$$
where
- $X$ is a closed and bounded set, $m \in \mathbb{N}$
- $\rho > 0$ 

---

## spiking neural networks as LSMs

A spiking neural network can be thought of as a liquid state machine.

**Reservoir computing** is inspired by the LSM model of computation, and uses highly recurrent spiking neural networks to perform computation. A readout mechanism is employed to map the state of the high dimensional spiking neural network to some desired output. Training, in reservoir computing, occurs at this readout step - while the dynamics of the internal 'reservoir' are left alone.

Maas (2002) modeled *plastic* synapses (see (Tsodyks, 2000) in an snn of 135 leaky integrate-and-fire neurons on a 3D spatial grid, with random, distance-dependent connection probability, as follows.
$$\frac{dR}{dt} = \frac{I}{\tau_{rec}}$$
- above describes changing resistance due to short term depression time constant. short term facilitation was also modeled.
$$\frac{dE}{dt} = - \frac{E}{\tau_{inact}} + U \cdot R \cdot \delta(t - t_{spike}) $$
- above describes changing effective use of synaptic resources due to inactivation time constant. changing effective use in active state, and recovered state, were also modeled.
- $U$ is a use parameter, describing amount of resources available.

'Readout' was implemented as another layer of 51 integrate-and-fire neurons. Readout weights were trained for different tasks, according to a perceptron-like learning rule.

Maas found internal state to deviate quickly, from varying Poisson input streams. After 30 ms, distance between internal states was roughly proportional to distance between corresponding input streams.

Maas found readout neurons able to generate robust and stable meaningful output from internal transient states, for example prediction of input stream patterns. 
- this was a nice theoretical result, since showed how the current internal perturbed state has inherent memory of historic inputs.

Maas found that describing computation inside an snn as an LSM allows a notion of parallel computing. Consider varying readout layers $r_i$ that each perform some $f_{r_i}^M$. Thus, parallel tasks can be performed, from same transient internal state of one network.